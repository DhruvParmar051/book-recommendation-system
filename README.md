# ğŸ“š Book Recommendation System  
### End-to-End ETL Pipeline + Google Books Enrichment + FastAPI Service  
**A Complete Data Engineering Project**

---

## 1. Introduction & Motivation

Modern recommendation systems rely on **clean, structured, and enriched data**.  
Raw library or OPAC datasets are often:

- Inconsistent in schema
- Noisy and duplicated
- Missing semantic metadata (authors, categories, descriptions)

This project addresses those challenges by building a **production-style data pipeline**
that transforms raw CSV book records into a **queryable, enriched database**, and exposes
the data through a **REST API** for downstream applications such as:

- Book recommendation systems
- Semantic search
- Analytics dashboards
- LLM-based applications

The project follows **real-world data engineering principles**:
- Clear separation of pipeline stages
- Deterministic and resume-safe processing
- CLI-driven configuration
- Database-backed persistence
- API-based data access

---

## 2. High-Level Capabilities

This system provides:

- ğŸ“¥ Robust ingestion of raw CSV files
- ğŸ§¹ Data cleaning, normalization, and deduplication
- ğŸŒ External enrichment via Google Books API
- ğŸ’¾ Persistent storage using SQLite
- ğŸš€ FastAPI service for browsing and searching books
- ğŸ§ª Fully self-documenting CLI (`--help` on every stage)

---

## 3. Project Structure & Responsibilities

Each folder has **one clear responsibility**, mirroring how production pipelines are organized.

```
book-recommendation-system/
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ ingestion.py
â”œâ”€â”€ clean/
â”‚   â””â”€â”€ clean.py
â”œâ”€â”€ transformation/
â”‚   â””â”€â”€ transformation.py
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ db.py
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ ingested_data/
â”‚   â”œâ”€â”€ clean_data/
â”‚   â”œâ”€â”€ enriched_data/
â”‚   â””â”€â”€ storage_data/
â””â”€â”€ README.md
```

This structure ensures:
- Clear data lineage
- Easy debugging
- Independent execution of each stage

---

## 4. ğŸ”½ Pipeline Architecture (End-to-End Flow)

The pipeline is **linear, deterministic, and restartable**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw CSV Files      â”‚
â”‚  (Untrusted Input)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INGESTION                  â”‚
â”‚ - Standardize schema       â”‚
â”‚ - Ensure required columns  â”‚
â”‚ - Minimal type fixes       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLEANING                   â”‚
â”‚ - Normalize text           â”‚
â”‚ - Validate ISBNs           â”‚
â”‚ - Deduplicate records      â”‚
â”‚ - Generate stable IDs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRANSFORMATION             â”‚
â”‚ - Google Books API         â”‚
â”‚ - Multithreaded enrichment â”‚
â”‚ - Resume-safe execution    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STORAGE                    â”‚
â”‚ - JSON â†’ SQLite            â”‚
â”‚ - Duplicate-safe inserts   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASTAPI SERVICE            â”‚
â”‚ - Browse books             â”‚
â”‚ - Search & lookup          â”‚
â”‚ - Trigger pipeline         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Running the Complete Pipeline

To execute **all ETL stages in order**, run:

```
python pipeline/main.py
```

### What Happens Internally

1. Ingests raw CSV files  
2. Cleans and deduplicates records  
3. Enriches books using Google Books API  
4. Stores final results in SQLite  

### Final Artifact

```
data/storage_data/books.sqlite
```

This database becomes the **single source of truth** for the API.

---

## 6. Detailed Stage Explanations

---

### 6.1 Ingestion Stage

**Goal:**  
Convert raw, inconsistent CSV files into a **canonical schema**.

**Key Design Choices**
- No cleaning or deduplication (by design)
- Schema-first approach
- Fail-safe handling of missing columns

**Why this matters:**  
Keeps ingestion lightweight and repeatable, deferring complex logic to later stages.

**Default Run**

```
python ingestion/ingestion.py
```

**Custom Input / Output**

```
python ingestion/ingestion.py \
  --input-dir my_raw_csvs \
  --output-dir my_ingested_csvs
```

---

### 6.2 Cleaning Stage

**Goal:**  
Improve data quality and remove redundancy.

**Operations**
- Normalize text (lowercase, trim, whitespace fix)
- Validate ISBNs (10/13-digit)
- Drop records without titles
- Deduplicate:
  - ISBN-based (preferred)
  - Title + Author fallback
- Generate stable `record_id` using hashing

**Why this matters:**  
Downstream enrichment and storage rely on **high-quality, unique records**.

**Default Run**

```
python clean/clean.py
```

**Custom Input / Output**

```
python clean/clean.py \
  --input-dir data/ingested_data \
  --output-file output/clean_books.csv
```

---

### 6.3 Transformation (Enrichment) Stage

**Goal:**  
Add semantic metadata using Google Books API.

**Enrichment Strategy**
1. Search by ISBN (highest precision)
2. Fallback to title + author search

**Reliability Features**
- Multithreading with controlled concurrency
- Hard API timeouts
- Incremental atomic saves
- Resume-safe after interruption

**Why this matters:**  
External APIs are unreliableâ€”this design prevents data loss and freezes.

**Default Run**

```
python transformation/transformation.py
```

**Custom Input / Output**

```
python transformation/transformation.py \
  --input-csv clean.csv \
  --output-json enriched.json
```

---

### 6.4 Storage Stage

**Goal:**  
Persist enriched records in a **query-efficient format**.

**Design Choices**
- SQLite (simple, portable, zero-config)
- Fixed schema
- `INSERT OR IGNORE` to prevent duplicates
- JSON serialization for list fields

**Why SQLite?**
- Ideal for smallâ€“medium datasets
- Easy integration with FastAPI
- No external service required

**Default Run**

```
python storage/db.py
```

**Custom Input / Output**

```
python storage/db.py \
  --input-json enriched.json \
  --output-db books.sqlite
```

---

## 7. FastAPI Service

The FastAPI layer provides **read-only access** to the final dataset.

### Key Endpoints

- `GET /books/` â€“ Paginated book listing  
- `GET /books/isbn/{isbn}` â€“ ISBN lookup  
- `GET /search/?q=term` â€“ Full-text search  
- `POST /sync/` â€“ Trigger pipeline in background  

### Start API Server

```
uvicorn api.main:app --reload
```

### Available Endpoints

- `GET /books/`
- `GET /books/isbn/{isbn}`
- `GET /search/?q=term`
- `POST /sync/` â€“ trigger pipeline in background

Swagger UI:
http://localhost:8000/docs

---

## 8. Data Dictionary (Core Fields)

| Field | Description |
|------|------------|
| record_id | Stable hashed identifier |
| book_key | Unique composite key |
| status | FOUND / MISSING |
| title | Normalized title |
| authors | Author list (JSON) |
| isbn | Valid ISBN |
| year | Publication year |
| subjects | Category list |
| summary | Description |
| publisher | Publisher name |

---

## 9. Design Philosophy

This project emphasizes:

- **Separation of concerns**
- **Reproducibility**
- **Operational safety**
- **Explainability**
- **Real-world engineering patterns**

It is intentionally designed to be:
- Extendable to Airflow / Prefect
- Ready for vector embeddings
- Suitable for ML & LLM pipelines

---

## 10. Future Enhancements

- Sentence Transformer embeddings
- Vector search (FAISS / Qdrant)
- Recommendation engine
- API authentication
- Caching & rate limiting
- Docker & CI/CD
- Data quality dashboards

---

## 11. Conclusion

This project demonstrates a **complete, production-style data pipeline**:

- âœ… Modular ETL design  
- âœ… Dynamic, CLI-driven execution  
- âœ… Resume-safe enrichment  
- âœ… Persistent storage  
- âœ… API-based data access  

It forms a strong foundation for **semantic search and recommendation systems**.

---

ğŸ‰ **Great work building a full data engineering system â€” all the best!**
