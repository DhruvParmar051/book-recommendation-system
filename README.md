# ðŸ“š Book Recommendation System  
### End-to-End ETL Pipeline + Google Books Enrichment + FastAPI Service  
**A Complete Data Engineering Project**

---

## 1. Introduction & Motivation

Modern recommendation systems rely on **clean, structured, and enriched data**.  
Raw library or OPAC datasets are often:

- Inconsistent in schema
- Noisy and duplicated
- Missing semantic metadata (authors, categories, descriptions)

This project addresses those challenges by building a **production-style data pipeline**
that transforms raw CSV book records into a **queryable, enriched database**, and exposes
the data through a **REST API** for downstream applications such as:

- Book recommendation systems
- Semantic search
- Analytics dashboards
- LLM-based applications

The project follows **real-world data engineering principles**:
- Clear separation of pipeline stages
- Deterministic and resume-safe processing
- CLI-driven configuration
- Database-backed persistence
- API-based data access

---

## 2. High-Level Capabilities

This system provides:

- ðŸ“¥ Robust ingestion of raw CSV files  
- ðŸ§¹ Data cleaning, normalization, and deduplication  
- ðŸŒ External enrichment via Google Books API  
- ðŸ’¾ Persistent storage using SQLite  
- ðŸš€ FastAPI service for browsing and searching books  
- ðŸ§ª Fully self-documenting CLI (`--help` on every stage)  

---

## 3. Project Structure & Responsibilities

Each folder has **one clear responsibility**, mirroring how production pipelines are organized.

```
book-recommendation-system/
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ ingestion.py
â”œâ”€â”€ clean/
â”‚   â””â”€â”€ clean.py
â”œâ”€â”€ transformation/
â”‚   â””â”€â”€ transformation.py
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ db.py
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ ingested_data/
â”‚   â”œâ”€â”€ clean_data/
â”‚   â”œâ”€â”€ enriched_data/
â”‚   â””â”€â”€ storage_data/
â””â”€â”€ README.md
```

This structure ensures:
- Clear data lineage
- Easy debugging
- Independent execution of each stage

---

## 4. ðŸ”½ Pipeline Architecture (End-to-End Flow)

The pipeline is **linear, deterministic, and restartable**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw CSV Files      â”‚
â”‚  (Untrusted Input)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INGESTION                  â”‚
â”‚ - Standardize schema       â”‚
â”‚ - Ensure required columns  â”‚
â”‚ - Minimal type fixes       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLEANING                   â”‚
â”‚ - Normalize text           â”‚
â”‚ - Validate ISBNs           â”‚
â”‚ - Deduplicate records      â”‚
â”‚ - Generate stable IDs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRANSFORMATION             â”‚
â”‚ - Google Books API         â”‚
â”‚ - Multithreaded enrichment â”‚
â”‚ - Resume-safe execution    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STORAGE                    â”‚
â”‚ - JSON â†’ SQLite            â”‚
â”‚ - Duplicate-safe inserts   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASTAPI SERVICE            â”‚
â”‚ - Browse books             â”‚
â”‚ - Search & lookup          â”‚
â”‚ - Trigger pipeline         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Running the Complete Pipeline

To execute **all ETL stages in order**, run:

```
python pipeline/main.py
```

### Final Artifact

```
data/storage_data/books.sqlite
```

This database becomes the **single source of truth** for the API.

---

## 6. ðŸ“Š Pipeline Statistics & Data Quality Report

This section quantitatively demonstrates **how data quality improves at each stage**.
All statistics were computed using an independent Jupyter Notebook (`.ipynb`)
to keep analysis separate from pipeline logic.

---

### 6.1 Raw Data Statistics (Before ETL)

| Metric | Value |
|------|------:|
| Total raw rows | **36,364** |
| Unique titles | **30,906** |
| Missing titles | **0** |
| Missing ISBNs | **412** |

**Observation**
- Raw data already contains duplicates
- ISBN coverage is high, but not complete
- No missing titles, indicating good upstream data collection

---

### 6.2 Ingestion Stage Statistics

| Metric | Value |
|------|------:|
| Total ingested rows | **36,364** |
| Unique titles | **30,906** |
| Unique ISBNs | **31,546** |
| Missing ISBNs | **412** |
| Missing year values | **170** |

**Observation**
- Ingestion preserves row count exactly
- No records are dropped
- Schema standardization does not alter data semantics

---

### 6.3 Cleaning Stage Statistics

| Metric | Value |
|------|------:|
| Total cleaned rows | **31,946** |
| Unique record_id | **31,946** |
| Unique ISBNs | **26,871** |
| Missing ISBNs | **5,075** |
| Duplicate records removed | **4,418** |

#### Deduplication Breakdown

| Category | Count |
|-------|------:|
| ISBN-based books | **26,871** |
| Non-ISBN books | **5,075** |

**Observation**
- Cleaning removes ~12% duplicate/noisy records
- ISBN-based deduplication is dominant
- Non-ISBN books are preserved using title+author logic

---

### 6.4 Enrichment (Google Books API) Statistics

| Metric | Value |
|------|------:|
| Total processed books | **31,946** |
| Successfully enriched | **9,221** |
| Missing enrichment | **22,725** |
| Enrichment success rate | **28.86%** |

#### Metadata Coverage (FOUND records)

| Field | Available |
|-----|---------:|
| Authors | **8,348** |
| Subjects | **8,497** |
| Summary | **7,313** |
| Publisher | **6,708** |

**Observation**
- ISBN-based enrichment significantly improves success rate
- Missing records are expected due to:
  - Old publications
  - Regional books
  - Limited Google Books coverage
- Pipeline safely records failures without data loss

---

### 6.5 Final Dataset Statistics (Post-Storage)

| Metric | Value |
|------|------:|
| Final books stored | **31,895** |
| Unique titles | **30,246** |
| Unique ISBNs | **26,026** |

**Observation**
- Final dataset is consistent and analytics-ready
- Duplicate-safe inserts prevent data corruption
- Slight reduction due to unique `book_key` constraint

---

### 6.6 Pipeline Row Count Summary

| Stage | Rows |
|------|-----:|
| Raw | 36,364 |
| Ingested | 36,364 |
| Cleaned | 31,946 |
| Enriched | 31,946 |

**Key Insight**
> Each pipeline stage has a **measurable, justified impact**, proving correctness
> and intentional data transformation rather than accidental data loss.

---

## 7. FastAPI Service

The FastAPI layer provides **read-only access** to the final dataset.

### Key Endpoints

- `GET /books/` â€“ Paginated book listing  
- `GET /books/isbn/{isbn}` â€“ ISBN lookup  
- `GET /search/?q=term` â€“ Full-text search  
- `POST /sync/` â€“ Trigger pipeline in background  

Swagger UI:
http://localhost:8000/docs

---

## 8. Data Dictionary (Core Fields)

| Field | Description |
|------|------------|
| record_id | Stable hashed identifier |
| book_key | Unique composite key |
| status | FOUND / MISSING |
| title | Normalized title |
| authors | Author list (JSON) |
| isbn | Valid ISBN |
| year | Publication year |
| subjects | Category list |
| summary | Description |
| publisher | Publisher name |

---

## 9. Design Philosophy

This project emphasizes:

- **Separation of concerns**
- **Reproducibility**
- **Operational safety**
- **Explainability**
- **Real-world engineering patterns**

---

## 10. Conclusion

This project demonstrates a **complete, production-style data pipeline**:

- âœ… Quantifiable data-quality improvements  
- âœ… Deterministic ETL stages  
- âœ… Resume-safe enrichment  
- âœ… Persistent storage  
- âœ… API-based data access  

It forms a strong foundation for **semantic search and recommendation systems**.

---

ðŸŽ‰ **Excellent work â€” this README now clearly proves engineering depth and data impact.**
